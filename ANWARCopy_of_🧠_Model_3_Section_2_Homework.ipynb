{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ABUALHUSSEIN/simple-rag-pipeline/blob/main/ANWARCopy_of_%F0%9F%A7%A0_Model_3_Section_2_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Model 3 Section 1: Homework\n",
        "> ## Building a Full RAG Pipeline\n",
        "\n",
        "> **üéØ Today‚Äôs Goal**: Combine all the parts from Section 1. We will use the **Retriever** (MiniLM) and the **Generator** (DistilBERT) to build a complete, end-to-end Retrieval-Augmented Generation (RAG) system.\n",
        "\n",
        "---\n",
        "\n",
        "###  recap: The Two Parts of Our RAG System\n",
        "\n",
        "1.  **The Retriever (Part 2)** üîé\n",
        "    * **Model:** `all-MiniLM-L6-v2`\n",
        "    * **Job:** To turn a text query into a vector and use **semantic search** to find the *most relevant* piece of context from our knowledge base.\n",
        "\n",
        "2.  **The Generator (Part 3)** ‚úçÔ∏è\n",
        "    * **Model:** `distilbert-base-cased-distilled-squad`\n",
        "    * **Job:** To take a `question` and a `context` and **extract** the specific answer from within the context.\n",
        "\n",
        "Today, we connect them. The output of the Retriever becomes the input for the Generator.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Bh20Fd7ypN9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### üß† Step 1: Load Models and Knowledge\n",
        "\n",
        "Now, let's load both of our specialized models and define our knowledge base. We have one model for retrieving and one for generating.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vz8_wQuapVMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 2: Load Models (The \"CPU/GPU Split\" Fix)\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# --- 1. Load the Retriever (MiniLM) on the CPU ---\n",
        "# We explicitly tell it to use the 'cpu'.\n",
        "# This is fast enough for a retriever and saves all our VRAM.\n",
        "retriever_model = SentenceTransformer(\n",
        "    'all-MiniLM-L6-v2',\n",
        "    device='cpu'  # Force to CPU\n",
        ")\n",
        "print(f\"‚úÖ Retriever model (MiniLM) loaded. Using device: cpu\")\n",
        "\n",
        "\n",
        "# --- 2. Load the Generator (DistilBERT) on the GPU ---\n",
        "# We check if a GPU is available and set the device index\n",
        "# 0 = first GPU, -1 = CPU\n",
        "pipeline_device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "generator_model = pipeline(\"question-answering\",\n",
        "                           model=\"distilbert-base-cased-distilled-squad\",\n",
        "                           device=pipeline_device) # Use GPU if available\n",
        "\n",
        "print(f\"‚úÖ Generator model (DistilBERT) loaded.\")\n",
        "if pipeline_device == 0:\n",
        "    print(\"   -> Running on GPU (Good!)\")\n",
        "else:\n",
        "    print(\"   -> WARNING: Running on CPU (Will be slow, but should work)\")"
      ],
      "metadata": {
        "id": "vp1iXwIQpZn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### üìö Step 2: Define and Encode Knowledge Base\n",
        "\n",
        "Here is our simple knowledge base. We will give this \"long-term memory\" to our AI agent.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VYrcM9FrpcAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 3: Define Knowledge Base\n",
        "# This is the \"long-term memory\" of our agent\n",
        "knowledge_base = [\n",
        "    \"Buddy is a 3-year-old Golden Retriever who loves to play fetch.\",\n",
        "    \"The capital of France is Paris, which is known for the Eiffel Tower.\",\n",
        "    \"Python is an interpreted, high-level, general-purpose programming language.\",\n",
        "    \"The first person to walk on the Moon was Neil Armstrong in 1969.\",\n",
        "    \"Climate change is the long-term alteration of temperature and typical weather patterns.\"\n",
        "]\n",
        "\n",
        "print(f\"üìö Knowledge base created with {len(knowledge_base)} documents.\")"
      ],
      "metadata": {
        "id": "iOB-pcM3pedx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ‚úèÔ∏è Task 1: Encode Your Knowledge\n",
        "\n",
        "Your first task is to use the **Retriever model** (`retriever_model`) to encode all the documents in your `knowledge_base`.\n",
        "\n",
        "**Your Goal:** Create a variable called `knowledge_embeddings` that holds the vector representations of all your documents.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rW8H65Pfph0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 4: Task 1 - Encode Knowledge\n",
        "print(\"--- Task 1: Encoding Knowledge Base ---\")\n",
        "\n",
        "# TODO: Use the retriever_model to encode the 'knowledge_base' list\n",
        "# The .encode() method takes a list of strings and returns a list of embeddings\n",
        "# Set convert_to_tensor=True\n",
        "knowledge_embeddings = retriever_model.encode(knowledge_base, convert_to_tensor=True)\n",
        "\n",
        "# --- Verification ---\n",
        "if 'knowledge_embeddings' in locals() and knowledge_embeddings.shape[0] == len(knowledge_base):\n",
        "    print(\"‚úÖ Success! Knowledge base has been encoded.\")\n",
        "    print(f\"   -> Embedding shape: {knowledge_embeddings.shape}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Task 1 not complete. 'knowledge_embeddings' not found or has wrong shape.\")"
      ],
      "metadata": {
        "id": "joVtNtTGHPOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ‚úèÔ∏è Task 2: Build the Retriever Function\n",
        "\n",
        "Now, let's build a function that performs the \"R\" (Retrieval) step. This function will take a user's `query` and find the most relevant document from our `knowledge_base`.\n",
        "\n",
        "**Your Goal:** Complete the `retrieve_context` function.\n",
        "1.  Encode the incoming `query` using the `retriever_model`.\n",
        "2.  Use `util.pytorch_cos_sim` to compare the `query_embedding` to all `knowledge_embeddings`.\n",
        "3.  Find the index of the highest-scoring document (use `torch.argmax`).\n",
        "4.  Return the *text* of that document from the `knowledge_base`.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nJLvCx32pnX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 5: Task 2 - Build the Retriever\n",
        "print(\"--- Task 2: Building the Retriever ---\")\n",
        "\n",
        "def retrieve_context(query):\n",
        "    # 1. Encode the query\n",
        "    # TODO: Encode the 'query' using the 'retriever_model'.\n",
        "    # Don't forget convert_to_tensor=True\n",
        "    query_embedding = retriever_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # 2. Compute cosine similarity\n",
        "    # TODO: Use 'util.pytorch_cos_sim' to compare the 'query_embedding'\n",
        "    # with all 'knowledge_embeddings'\n",
        "    # This returns a tensor of scores, one for each document\n",
        "    cos_scores = util.pytorch_cos_sim(query_embedding, knowledge_embeddings)\n",
        "\n",
        "    # 3. Find the best match\n",
        "    # TODO: Use 'torch.argmax' to find the index of the highest score\n",
        "    # The highest score corresponds to the most similar document\n",
        "    top_result_index = torch.argmax(cos_scores)\n",
        "\n",
        "    # 4. Return the matching document text\n",
        "    # TODO: Return the text from 'knowledge_base' at 'top_result_index'\n",
        "    return knowledge_base[top_result_index]\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Testing retrieve_context('What is Python?')...\")\n",
        "retrieved = retrieve_context(\"What is Python?\")\n",
        "print(f\"   -> Retrieved: '{retrieved}'\")\n",
        "if retrieved and \"Python\" in retrieved:\n",
        "    print(\"‚úÖ Success! Retriever function works.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Retriever function failed to find the right document.\")"
      ],
      "metadata": {
        "id": "ACExUaG8Kr3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ‚úèÔ∏è Task 3: Build the Generator Function\n",
        "\n",
        "Great! We have a function to get context. Now let's build a function for the \"G\" (Generation) step. This function will take a `question` and the `context` we just retrieved.\n",
        "\n",
        "**Your Goal:** Complete the `generate_answer` function.\n",
        "1.  Call the `generator_model` (which is a `pipeline` object).\n",
        "2.  Pass the `question` and `context` to it.\n",
        "3.  Return *only the answer* from the resulting dictionary (e.g., `result['answer']`).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ejGGnWtQprE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 6: Task 3 - Build the Generator\n",
        "print(\"\\n--- Task 3: Building the Generator ---\")\n",
        "\n",
        "def generate_answer(question, context):\n",
        "    # 1. Call the pipeline\n",
        "    # TODO: Call the 'generator_model' pipeline, passing in the\n",
        "    # 'question' and 'context'\n",
        "    result = generator_model(question=question, context=context)\n",
        "\n",
        "    # 2. Return the answer\n",
        "    # TODO: Return the 'answer' part of the 'result' dictionary\n",
        "    return result['answer']\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Testing generate_answer('What is Python?', '...')...\")\n",
        "test_context = \"Python is a popular programming language.\"\n",
        "test_question = \"What is Python?\"\n",
        "answer = generate_answer(test_question, test_context)\n",
        "print(f\"   -> Question: '{test_question}'\")\n",
        "print(f\"   -> Context: '{test_context}'\")\n",
        "print(f\"   -> Answer: '{answer}'\")\n",
        "\n",
        "if answer and \"popular programming language\" in answer:\n",
        "    print(\"‚úÖ Success! Generator function works.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Generator function failed to extract the answer.\")"
      ],
      "metadata": {
        "id": "LEEe15aFMN8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### üöÄ Task 4: Build the Full RAG Pipeline!\n",
        "\n",
        "This is the final step. Let's combine our two functions into a single, end-to-end RAG pipeline. This function will orchestrate the entire process.\n",
        "\n",
        "**Your Goal:** Complete the `ask_rag_pipeline` function.\n",
        "1.  Call your `retrieve_context` function to get the `best_context` for the `query`.\n",
        "2.  Call your `generate_answer` function, passing in the *original* `query` and the `best_context` you just found.\n",
        "3.  Return the final `answer`.\n",
        "\n",
        "After you write the function, we'll test it with a query!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WXuKM7bHpyM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 7: Task 4 - Build the Full RAG Pipeline\n",
        "print(\"\\n--- Task 4: Building the Full RAG Pipeline ---\")\n",
        "\n",
        "def ask_rag_pipeline(query):\n",
        "    # 1. Retrieve\n",
        "    # TODO: Call your 'retrieve_context' function\n",
        "    # This finds the most relevant document for the query.\n",
        "    best_context = retrieve_context(query)\n",
        "\n",
        "    # 2. Generate\n",
        "    # TODO: Call your 'generate_answer' function\n",
        "    # This uses the retrieved document to extract the specific answer.\n",
        "    final_answer = generate_answer(question=query, context=best_context)\n",
        "\n",
        "    # 3. Return both the answer and the context for inspection\n",
        "    return final_answer, best_context\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Testing the full RAG pipeline...\")\n",
        "query = \"What is the capital of France?\"\n",
        "print(f\"Query: '{query}'\")\n",
        "\n",
        "# We expect two return values, so we unpack them like this\n",
        "final_answer, retrieved_context = ask_rag_pipeline(query)\n",
        "\n",
        "print(f\"   -> Retrieved Context: '{retrieved_context}'\")\n",
        "print(f\"   -> Final Answer: '{final_answer}'\")\n",
        "\n",
        "if final_answer and final_answer.lower() == \"paris\":\n",
        "    print(\"‚úÖ Success! Your RAG pipeline is working!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è RAG pipeline failed. Expected 'Paris'.\")\n",
        "\n",
        "# --- Let's try another one! ---\n",
        "print(\"\\n--- Another Test ---\")\n",
        "query_2 = \"Who was the first person on the moon?\"\n",
        "print(f\"Query: '{query_2}'\")\n",
        "answer_2, context_2 = ask_rag_pipeline(query_2)\n",
        "print(f\"   -> Retrieved Context: '{context_2}'\")\n",
        "print(f\"   -> Final Answer: '{answer_2}'\")\n",
        "if answer_2 and \"neil armstrong\" in answer_2.lower():\n",
        "    print(\"‚úÖ Correctly answered the second question!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Failed on the second question.\")"
      ],
      "metadata": {
        "id": "DWihyGnpNCx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### üß™ Self-Assessment\n",
        "\n",
        "Run this final cell to test your complete RAG pipeline. This assessment will:\n",
        "1.  Add new information to the agent's knowledge base.\n",
        "2.  Ask questions that *require* the RAG pipeline to work.\n",
        "3.  It will check if your `retrieve_context` function finds the right document AND if your `generate_answer` function extracts the correct answer.\n",
        "\n",
        "Good luck!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KmDt8jCyp32R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Code Cell 8: Self-Assessment\n",
        "print(\"\\n--- üß™ Self-Assessment ---\")\n",
        "\n",
        "# We will add new documents to the knowledge base and test the agent.\n",
        "# This simulates expanding the agent's memory.\n",
        "\n",
        "try:\n",
        "    # --- Setup for Test ---\n",
        "    new_knowledge = [\n",
        "        \"The currency of Japan is the Yen.\",\n",
        "        \"Maverick is a clever Border Collie who knows many tricks.\",\n",
        "        \"The highest mountain in the world is Mount Everest.\"\n",
        "    ]\n",
        "    # Update all the pieces:\n",
        "    # 1. Update the text list\n",
        "    knowledge_base.extend(new_knowledge)\n",
        "    # 2. Update the embeddings (re-encode everything)\n",
        "    knowledge_embeddings = retriever_model.encode(knowledge_base, convert_to_tensor=True)\n",
        "\n",
        "    print(f\"üìö Agent memory updated. Total documents: {len(knowledge_base)}\")\n",
        "\n",
        "    # --- Test Cases ---\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"query\": \"What is the currency of Japan?\",\n",
        "            \"expected_context_keyword\": \"Japan\",\n",
        "            \"expected_answer_keyword\": \"Yen\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"What kind of dog is Maverick?\",\n",
        "            \"expected_context_keyword\": \"Maverick\",\n",
        "            \"expected_answer_keyword\": \"Border Collie\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"Who was the first person on the Moon?\",\n",
        "            \"expected_context_keyword\": \"Moon\",\n",
        "            \"expected_answer_keyword\": \"Neil Armstrong\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_cases) * 2 # Each test has a retrieval and generation part\n",
        "\n",
        "    for i, test in enumerate(test_cases):\n",
        "        print(f\"\\n--- Test Case {i+1} ---\")\n",
        "        query = test[\"query\"]\n",
        "        print(f\"Query: \\\"{query}\\\"\")\n",
        "\n",
        "        # Run the full pipeline\n",
        "        answer, context = ask_rag_pipeline(query)\n",
        "\n",
        "        # Check Retrieval\n",
        "        print(f\"   -> üîé Retrieved: '{context}'\")\n",
        "        if test[\"expected_context_keyword\"] in context:\n",
        "            print(\"   -> ‚úÖ Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"   -> ‚ùå Retrieval Failed. Expected context with: '{test['expected_context_keyword']}'\")\n",
        "\n",
        "        # Check Generation\n",
        "        print(f\"   -> ‚úçÔ∏è Answer: '{answer}'\")\n",
        "        if test[\"expected_answer_keyword\"].lower() in answer.lower():\n",
        "            print(\"   -> ‚úÖ Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"   -> ‚ùå Generation Failed. Expected answer with: '{test['expected_answer_keyword']}'\")\n",
        "\n",
        "    # Final Score\n",
        "    print(f\"\\n--- üèÅ Assessment Complete ---\")\n",
        "    print(f\"üéØ Your Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"üéâüéâüéâ Perfect! You have successfully built and tested a full RAG pipeline!\")\n",
        "    elif score >= total // 2:\n",
        "        print(\"üëç Great job! Your pipeline is working. Review any failed tests to see what happened.\")\n",
        "    else:\n",
        "        print(\"üîß Keep trying! Check your functions in Tasks 2, 3, and 4.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- ‚ö†Ô∏è Assessment Failed ---\")\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please check your code in all tasks and try again.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MwKVfPYYp7Pd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}